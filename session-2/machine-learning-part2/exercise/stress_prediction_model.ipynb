{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress Level Prediction - Improved Decision Tree Classifier\n",
    "\n",
    "This notebook demonstrates stress level prediction using decision tree classification with hyperparameter tuning and comprehensive analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STRESS LEVEL PREDICTION - IMPROVED DECISION TREE CLASSIFIER\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('sleep_heath_lifecycle_dataset.csv')\n",
    "df = df.drop(columns=['Person ID'])\n",
    "df.columns = [c.strip().lower().replace(' ', '_') for c in df.columns]\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Target variable (stress_level) distribution:\\n{df['stress_level'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split blood pressure into systolic and diastolic\n",
    "bp = df['blood_pressure'].str.split('/', expand=True)\n",
    "df['systolic_bp'] = pd.to_numeric(bp[0], errors='coerce')\n",
    "df['diastolic_bp'] = pd.to_numeric(bp[1], errors='coerce')\n",
    "df = df.drop(columns='blood_pressure')\n",
    "\n",
    "# Fill missing sleep disorder values\n",
    "df['sleep_disorder'] = df['sleep_disorder'].fillna('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categoricals\n",
    "X = df.drop(columns='stress_level')\n",
    "y = df['stress_level']\n",
    "categorical = X.select_dtypes('object').columns\n",
    "X = pd.get_dummies(X, columns=categorical, drop_first=True)\n",
    "\n",
    "print(f\"\\nFeatures after encoding: {X.shape[1]}\")\n",
    "print(f\"Feature names: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Standardize Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize numeric features\n",
    "num_cols = X.select_dtypes(['int64', 'float64']).columns\n",
    "scaler = StandardScaler()\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING WITH GRID SEARCH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "dt_base = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "print(\"\\nPerforming Grid Search (this may take a moment)...\")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt_base,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nâœ“ Grid Search Complete!\")\n",
    "print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Comparison: Base vs Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train base model (default parameters)\n",
    "base_model = DecisionTreeClassifier(random_state=42)\n",
    "base_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_base = base_model.predict(X_test)\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracies\n",
    "base_accuracy = accuracy_score(y_test, y_pred_base)\n",
    "tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
    "\n",
    "print(f\"\\n{'Model':<25} {'Train Accuracy':<20} {'Test Accuracy':<20}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Base model scores\n",
    "base_train_acc = base_model.score(X_train, y_train)\n",
    "print(f\"{'Base Model':<25} {base_train_acc:<20.4f} {base_accuracy:<20.4f}\")\n",
    "\n",
    "# Tuned model scores\n",
    "tuned_train_acc = best_model.score(X_train, y_train)\n",
    "print(f\"{'Tuned Model':<25} {tuned_train_acc:<20.4f} {tuned_accuracy:<20.4f}\")\n",
    "\n",
    "improvement = ((tuned_accuracy - base_accuracy) / base_accuracy) * 100\n",
    "print(f\"\\nðŸŽ¯ Improvement: {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation scores\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Cross-Validation Scores (5-fold):\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "cv_scores_base = cross_val_score(\n",
    "    base_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "cv_scores_tuned = cross_val_score(\n",
    "    best_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"\\nBase Model CV Accuracy: {cv_scores_base.mean():.4f} (+/- {cv_scores_base.std() * 2:.4f})\")\n",
    "print(f\"Tuned Model CV Accuracy: {cv_scores_tuned.mean():.4f} (+/- {cv_scores_tuned.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Detailed Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n--- BASE MODEL ---\")\n",
    "print(f\"Accuracy: {base_accuracy:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_base))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "\n",
    "print(\"\\n--- TUNED MODEL ---\")\n",
    "print(f\"Accuracy: {tuned_accuracy:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_tuned))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get feature importances from tuned model\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Comprehensive Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance and model performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# 1. Feature Importance Bar Plot\n",
    "top_n = 15\n",
    "top_features = feature_importance.head(top_n)\n",
    "axes[0, 0].barh(range(len(top_features)), top_features['Importance'], color='steelblue')\n",
    "axes[0, 0].set_yticks(range(len(top_features)))\n",
    "axes[0, 0].set_yticklabels(top_features['Feature'])\n",
    "axes[0, 0].invert_yaxis()\n",
    "axes[0, 0].set_xlabel('Importance Score', fontweight='bold')\n",
    "axes[0, 0].set_title(f'Top {top_n} Feature Importances', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 2. Confusion Matrix - Tuned Model\n",
    "cm = confusion_matrix(y_test, y_pred_tuned)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1], cbar=True)\n",
    "axes[0, 1].set_xlabel('Predicted', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Actual', fontweight='bold')\n",
    "axes[0, 1].set_title('Confusion Matrix - Tuned Model', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 3. Model Comparison\n",
    "models_names = ['Base Model', 'Tuned Model']\n",
    "accuracies = [base_accuracy, tuned_accuracy]\n",
    "colors = ['lightcoral', 'lightgreen']\n",
    "\n",
    "bars = axes[1, 0].bar(models_names, accuracies, color=colors,\n",
    "                      alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1, 0].set_ylabel('Accuracy', fontweight='bold')\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "axes[1, 0].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                    f'{acc:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 4. Cross-Validation Score Comparison\n",
    "cv_data = pd.DataFrame({\n",
    "    'Model': ['Base']*5 + ['Tuned']*5,\n",
    "    'CV Score': list(cv_scores_base) + list(cv_scores_tuned)\n",
    "})\n",
    "\n",
    "box_plot = axes[1, 1].boxplot([cv_scores_base, cv_scores_tuned],\n",
    "                              labels=['Base Model', 'Tuned Model'],\n",
    "                              patch_artist=True,\n",
    "                              boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                              medianprops=dict(color='red', linewidth=2))\n",
    "axes[1, 1].set_ylabel('Cross-Validation Accuracy', fontweight='bold')\n",
    "axes[1, 1].set_title('Cross-Validation Scores Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('stress_classification_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Decision Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DECISION TREE VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plot_tree(best_model,\n",
    "          feature_names=X.columns,\n",
    "          class_names=[str(c) for c in best_model.classes_],\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10,\n",
    "          max_depth=3)  # Show only first 3 levels for readability\n",
    "plt.title('Decision Tree Structure (First 3 Levels)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('decision_tree_structure.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nâœ“ Best Model Parameters: {grid_search.best_params_}\")\n",
    "print(f\"âœ“ Base Model Test Accuracy: {base_accuracy:.4f}\")\n",
    "print(f\"âœ“ Tuned Model Test Accuracy: {tuned_accuracy:.4f}\")\n",
    "print(f\"âœ“ Performance Improvement: {improvement:+.2f}%\")\n",
    "print(f\"âœ“ Most Important Feature: {feature_importance.iloc[0]['Feature']}\")\n",
    "print(f\"âœ“ Top 3 Features: {', '.join(feature_importance.head(3)['Feature'].tolist())}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
